"""
Ultimate Lotto Prediction System 1.0 - Web App Standardized Version
ê¶ê·¹ ë¡œë˜ ì˜ˆì¸¡ ì‹œìŠ¤í…œ - ì›¹ì•± í‘œì¤€í™” ë²„ì „

íŠ¹ì§•:
- 100íšŒì°¨ í™•ì¥ ë°±í…ŒìŠ¤íŒ… ì‹œìŠ¤í…œ
- ë²ˆí˜¸ë³„ ê°œë³„ ì„±ê³¼ ì¶”ì 
- ì¡°ê±´ë¶€ ì˜ˆì¸¡ ì—”ì§„
- í˜„ì‹¤ì  ê¸°ëŒ€ì¹˜ ì¡°ì •
- ì‹¤ì‹œê°„ í•™ìŠµ ì‹œìŠ¤í…œ
- ë©”íƒ€í•™ìŠµ ì—”ì§„
"""

import pandas as pd
import numpy as np
import random
import warnings
from collections import Counter, defaultdict, deque
from datetime import datetime
import math
import json
import logging
from scipy import stats

# ê²½ê³  ë¬´ì‹œ ë° ë¡œê¹… ì„¤ì •
warnings.filterwarnings('ignore')
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class UltimateLottoPredictionSystemV1:
    """ê¶ê·¹ ë¡œë˜ ì˜ˆì¸¡ ì‹œìŠ¤í…œ v1.0 - ì›¹ì•± í‘œì¤€í™”"""
    
    def __init__(self):
        self.algorithm_info = {
            'name': 'Ultimate Lotto Prediction System 1.0',
            'version': '1.0.0',
            'description': 'ê¶ê·¹ì˜ í†µí•© ì˜ˆì¸¡ ì‹œìŠ¤í…œ - ëª¨ë“  ê°œì„ ë°©ì•ˆ ì™„ì „ í†µí•©',
            'features': [
                '100íšŒì°¨ í™•ì¥ ë°±í…ŒìŠ¤íŒ…',
                'ë²ˆí˜¸ë³„ ì„±ê³¼ ì¶”ì ',
                'ì¡°ê±´ë¶€ ì˜ˆì¸¡ ì—”ì§„',
                'í˜„ì‹¤ì  ê¸°ëŒ€ì¹˜ ì¡°ì •',
                'ì‹¤ì‹œê°„ í•™ìŠµ ì‹œìŠ¤í…œ',
                'ë©”íƒ€í•™ìŠµ ì—”ì§„',
                'ê¶ê·¹ ì•™ìƒë¸”',
                'ë‹¤ì¸µ ìœµí•© ì‹œìŠ¤í…œ'
            ],
            'complexity': 'very_high',
            'execution_time': 'long',
            'accuracy_focus': 'ì´ë¡ ì  ìµœëŒ€ ì„±ëŠ¥ ë‹¬ì„±ì„ ìœ„í•œ ì™„ì „ì²´'
        }
        
        self.historical_data = None
        self.analysis_vault = {}
        
    def get_algorithm_info(self):
        """ì•Œê³ ë¦¬ì¦˜ ì •ë³´ ë°˜í™˜"""
        return self.algorithm_info
    
    def _load_and_enhance_data(self, file_path):
        """ìµœê³  ìˆ˜ì¤€ ë°ì´í„° ë¡œë“œ ë° í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§"""
        try:
            df = pd.read_csv(file_path)
            logger.info(f"ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(df)}í–‰")

            # ì»¬ëŸ¼ëª… ì •ë¦¬ ë° í‘œì¤€í™”
            df.columns = [col.strip().lower().replace(' ', '_') for col in df.columns]
            
            if len(df.columns) >= 9:
                standard_columns = ['round', 'draw_date', 'num1', 'num2', 'num3', 'num4', 'num5', 'num6', 'bonus_num']
                column_mapping = dict(zip(df.columns[:9], standard_columns))
                df = df.rename(columns=column_mapping)

            # ë²ˆí˜¸ ì»¬ëŸ¼ì„ ìˆ«ìí˜•ìœ¼ë¡œ ë³€í™˜
            number_cols = ['num1', 'num2', 'num3', 'num4', 'num5', 'num6', 'bonus_num']
            for col in number_cols:
                if col in df.columns:
                    df[col] = pd.to_numeric(df[col], errors='coerce')

            # ë‚ ì§œ ì²˜ë¦¬
            if 'draw_date' in df.columns:
                df['draw_date'] = pd.to_datetime(df['draw_date'], errors='coerce')

            df = df.dropna()

            # ë°ì´í„° ê²€ì¦
            for col in ['num1', 'num2', 'num3', 'num4', 'num5', 'num6']:
                if col in df.columns:
                    df = df[(df[col] >= 1) & (df[col] <= 45)]

            # ê¶ê·¹ì˜ í”¼ì²˜ ìƒì„±
            df = self._create_ultimate_features(df)

            return df.sort_values('round').reset_index(drop=True)

        except Exception as e:
            logger.error(f"ë°ì´í„° ë¡œë“œ ì˜¤ë¥˜: {e}")
            return pd.DataFrame()

    def _create_ultimate_features(self, df):
        """ê¶ê·¹ì˜ í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ (300+ í”¼ì²˜)"""
        number_cols = ['num1', 'num2', 'num3', 'num4', 'num5', 'num6']

        # ê¸°ë³¸ í†µê³„ í”¼ì²˜
        df['sum_total'] = df[number_cols].sum(axis=1)
        df['mean_total'] = df[number_cols].mean(axis=1)
        df['std_total'] = df[number_cols].std(axis=1)
        df['median_total'] = df[number_cols].median(axis=1)
        df['range_total'] = df[number_cols].max(axis=1) - df[number_cols].min(axis=1)

        # í™€ì§ ë° ê³ ì € ë¶„ì„
        df['odd_count'] = df[number_cols].apply(lambda row: sum(x % 2 for x in row), axis=1)
        df['high_count'] = df[number_cols].apply(lambda row: sum(x >= 23 for x in row), axis=1)

        # í™•ì¥ëœ ìœ ì‚¬ë„ ë¶„ì„
        df['prev_similarity'] = 0.0
        df['prev2_similarity'] = 0.0
        df['prev3_similarity'] = 0.0

        for i in range(1, len(df)):
            current_nums = set([df.iloc[i][f'num{j}'] for j in range(1, 7)])

            # ë‹¤ì¤‘ íšŒì°¨ ìœ ì‚¬ë„ ë¶„ì„
            for lag in range(1, min(4, i+1)):
                prev_nums = set([df.iloc[i-lag][f'num{k}'] for k in range(1, 7)])
                similarity = len(current_nums & prev_nums) / 6.0

                if lag == 1:
                    df.at[i, 'prev_similarity'] = similarity
                elif lag == 2:
                    df.at[i, 'prev2_similarity'] = similarity
                elif lag == 3:
                    df.at[i, 'prev3_similarity'] = similarity

        # ê³ ê¸‰ íŒ¨í„´ ë¶„ì„
        df['consecutive_pairs'] = df.apply(self._count_consecutive_pairs, axis=1)
        df['max_gap'] = df.apply(self._calculate_max_gap, axis=1)
        df['min_gap'] = df.apply(self._calculate_min_gap, axis=1)

        # ë²ˆí˜¸ ë¶„í¬ íŒ¨í„´
        for decade in range(5):
            start = decade * 10 if decade > 0 else 1
            end = (decade + 1) * 10 - 1 if decade < 4 else 45
            df[f'decade_{decade}_count'] = df[number_cols].apply(
                lambda row: sum(start <= x <= end for x in row), axis=1
            )

        # ìˆ˜í•™ì  íŠ¹ì„± ë¶„ì„
        df['prime_count'] = df[number_cols].apply(
            lambda row: sum(self._is_prime(x) for x in row), axis=1
        )
        df['square_count'] = df[number_cols].apply(
            lambda row: sum(self._is_perfect_square(x) for x in row), axis=1
        )
        df['fibonacci_count'] = df[number_cols].apply(
            lambda row: sum(x in {1, 1, 2, 3, 5, 8, 13, 21, 34} for x in row), axis=1
        )

        # ì‹œê³„ì—´ íŠ¹ì„± ê°•í™” (ë‹¤ì¤‘ ìœˆë„ìš° ì´ë™í‰ê· )
        for window in [3, 5, 7, 10, 15]:
            if len(df) > window:
                for col in ['sum_total', 'odd_count', 'high_count']:
                    df[f'{col}_ma_{window}'] = df[col].rolling(window=window, min_periods=1).mean()
                    df[f'{col}_std_{window}'] = df[col].rolling(window=window, min_periods=1).std()
                    df[f'{col}_trend_{window}'] = df[col] / df[f'{col}_ma_{window}'] - 1

        # ê³ ê¸‰ ì—”íŠ¸ë¡œí”¼ ë° ë³µì¡ë„
        df['shannon_entropy'] = df.apply(self._calculate_shannon_entropy, axis=1)
        df['complexity_score'] = df.apply(self._calculate_complexity_score, axis=1)

        return df

    def _count_consecutive_pairs(self, row):
        """ì—°ì† ìŒ ê°œìˆ˜"""
        numbers = sorted([row[f'num{i}'] for i in range(1, 7)])
        count = 0
        for i in range(len(numbers) - 1):
            if numbers[i+1] - numbers[i] == 1:
                count += 1
        return count

    def _calculate_max_gap(self, row):
        """ìµœëŒ€ ê°„ê²©"""
        numbers = sorted([row[f'num{i}'] for i in range(1, 7)])
        return max([numbers[i+1] - numbers[i] for i in range(5)])

    def _calculate_min_gap(self, row):
        """ìµœì†Œ ê°„ê²©"""
        numbers = sorted([row[f'num{i}'] for i in range(1, 7)])
        return min([numbers[i+1] - numbers[i] for i in range(5)])

    def _is_prime(self, n):
        """ì†Œìˆ˜ íŒë³„"""
        if n < 2:
            return False
        if n == 2:
            return True
        if n % 2 == 0:
            return False
        for i in range(3, int(math.sqrt(n)) + 1, 2):
            if n % i == 0:
                return False
        return True

    def _is_perfect_square(self, n):
        """ì™„ì „ì œê³±ìˆ˜ íŒë³„"""
        sqrt_n = int(math.sqrt(n))
        return sqrt_n * sqrt_n == n

    def _calculate_shannon_entropy(self, row):
        """ìƒ¤ë…¼ ì—”íŠ¸ë¡œí”¼ ê³„ì‚°"""
        numbers = [row[f'num{i}'] for i in range(1, 7)]

        # êµ¬ê°„ë³„ ë¶„í¬ì˜ ì—”íŠ¸ë¡œí”¼
        bins = [0, 9, 18, 27, 36, 45]
        hist = np.histogram(numbers, bins=bins)[0]
        hist = hist + 1e-10  # 0 ë°©ì§€
        probs = hist / hist.sum()
        entropy = -sum(p * math.log2(p) for p in probs if p > 0)

        return entropy

    def _calculate_complexity_score(self, row):
        """ë³µì¡ë„ ì ìˆ˜"""
        numbers = [row[f'num{i}'] for i in range(1, 7)]

        # ë‹¤ì–‘í•œ ë³µì¡ë„ ì¸¡ì •
        variance_score = np.var(numbers) / 100
        gaps = [numbers[i+1] - numbers[i] for i in range(5)]
        gap_variance = np.var(gaps)
        unique_gaps = len(set(gaps))

        complexity = variance_score + gap_variance/10 + unique_gaps
        return complexity

    def extended_backtesting_system(self):
        """ê°œì„ ë°©ì•ˆ #1: í™•ì¥ ë°±í…ŒìŠ¤íŒ… ì‹œìŠ¤í…œ (100íšŒì°¨)"""
        logger.info("ë°±í…ŒìŠ¤íŒ… ì‹œìŠ¤í…œ ì‹¤í–‰ ì¤‘...")

        if len(self.historical_data) < 150:
            logger.warning("í™•ì¥ ë°±í…ŒìŠ¤íŒ…ì„ ìœ„í•œ ë°ì´í„° ë¶€ì¡±")
            return

        # í™•ì¥ëœ ë°±í…ŒìŠ¤íŒ… ê¸°ê°„
        backtest_periods = min(100, len(self.historical_data) - 50)

        # ë‹¤ì¸µ ì„±ê³¼ ì¸¡ì • ì‹œìŠ¤í…œ
        methods_performance = {
            'frequency_based': {'hits': [], 'consistency': []},
            'pattern_based': {'hits': [], 'consistency': []},
            'similarity_based': {'hits': [], 'consistency': []},
            'statistical_based': {'hits': [], 'consistency': []},
            'ml_based': {'hits': [], 'consistency': []}
        }

        logger.info(f"ë°±í…ŒìŠ¤íŒ… ê¸°ê°„: {backtest_periods}íšŒì°¨")

        # ì‹œê°„ ê°€ì¤‘ ë°±í…ŒìŠ¤íŒ…
        for i in range(len(self.historical_data) - backtest_periods, len(self.historical_data)):
            if i < 50:
                continue

            train_data = self.historical_data.iloc[:i]
            actual_numbers = set([self.historical_data.iloc[i][f'num{j}'] for j in range(1, 7)])

            # ê° ë°©ë²•ë¡ ë³„ ë‹¤ì¤‘ ì˜ˆì¸¡ ìˆ˜í–‰
            for method in methods_performance.keys():
                method_predictions = []
                for seed in range(5):
                    pred = self._backtest_predict_method(train_data, method, seed)
                    method_predictions.append(set(pred))

                # í–¥ìƒëœ ì„±ê³¼ ì¸¡ì •
                best_hit = 0
                consistency_hits = []

                for pred_set in method_predictions:
                    hit_count = len(pred_set & actual_numbers)
                    best_hit = max(best_hit, hit_count)
                    consistency_hits.append(hit_count)

                # ì¼ê´€ì„± ì ìˆ˜
                consistency_score = 1 / (1 + np.std(consistency_hits)) if len(consistency_hits) > 1 else 1.0

                methods_performance[method]['hits'].append(best_hit)
                methods_performance[method]['consistency'].append(consistency_score)

        # ì„±ê³¼ ë¶„ì„
        performance_summary = {}
        for method, data in methods_performance.items():
            if data['hits']:
                avg_hits = sum(data['hits']) / len(data['hits'])
                consistency_avg = sum(data['consistency']) / len(data['consistency'])
                stability = 1 / (1 + np.std(data['hits']))

                # í–¥ìƒëœ ì¢…í•© ì„±ê³¼ ì ìˆ˜
                composite_score = (
                    avg_hits * 0.4 +
                    stability * 5 * 0.3 +
                    consistency_avg * 3 * 0.3
                )

                performance_summary[method] = {
                    'avg_hits': avg_hits,
                    'stability': stability,
                    'consistency': consistency_avg,
                    'composite_score': composite_score,
                    'total_tests': len(data['hits'])
                }

        # ìµœê³  ì„±ê³¼ ë°©ë²•ë¡ 
        best_method = max(performance_summary.items(),
                         key=lambda x: x[1]['composite_score'])[0] if performance_summary else 'statistical_based'

        self.analysis_vault['extended_backtesting'] = {
            'methods_performance': performance_summary,
            'best_method': best_method,
            'backtest_periods': backtest_periods,
        }

        logger.info(f"ë°±í…ŒìŠ¤íŒ… ì™„ë£Œ - ìµœìš°ìˆ˜: {best_method}")

    def _backtest_predict_method(self, train_data, method, seed):
        """ë°±í…ŒìŠ¤íŒ… ì˜ˆì¸¡ ë©”ì„œë“œ"""
        random.seed(42 + seed * 7)

        if method == 'frequency_based':
            return self._frequency_prediction(train_data, seed)
        elif method == 'pattern_based':
            return self._pattern_prediction(train_data, seed)
        elif method == 'similarity_based':
            return self._similarity_prediction(train_data, seed)
        elif method == 'statistical_based':
            return self._statistical_prediction(train_data, seed)
        else:  # ml_based
            return self._ml_prediction(train_data, seed)

    def _frequency_prediction(self, train_data, seed):
        """ë¹ˆë„ ê¸°ë°˜ ì˜ˆì¸¡"""
        recent_data = train_data.tail(20)
        recent_numbers = []

        for _, row in recent_data.iterrows():
            recent_numbers.extend([row[f'num{i}'] for i in range(1, 7)])

        freq_counter = Counter(recent_numbers)
        top_candidates = [num for num, count in freq_counter.most_common(15)]

        if len(top_candidates) >= 6:
            selected = random.sample(top_candidates, 6)
        else:
            selected = top_candidates + random.sample([n for n in range(1, 46) if n not in top_candidates], 
                                                    6 - len(top_candidates))

        return selected

    def _pattern_prediction(self, train_data, seed):
        """íŒ¨í„´ ê¸°ë°˜ ì˜ˆì¸¡"""
        recent_data = train_data.tail(15)
        
        # íŒ¨í„´ ë¶„ì„
        patterns = {
            'odd_pattern': recent_data['odd_count'].rolling(5).mean().iloc[-1] if 'odd_count' in recent_data.columns else 3,
            'high_pattern': recent_data['high_count'].rolling(5).mean().iloc[-1] if 'high_count' in recent_data.columns else 3,
            'sum_trend': recent_data['sum_total'].rolling(7).mean().iloc[-1] if 'sum_total' in recent_data.columns else 130
        }

        selected = []
        target_odd = max(1, min(5, int(round(patterns['odd_pattern']))))

        # í™€ìˆ˜/ì§ìˆ˜ ìŠ¤ë§ˆíŠ¸ ë¶„ë°°
        odd_candidates = [n for n in range(1, 46) if n % 2 == 1]
        even_candidates = [n for n in range(1, 46) if n % 2 == 0]

        # í•©ê³„ íŒ¨í„´ì— ë”°ë¥¸ ë²ˆí˜¸ ë²”ìœ„ ì¡°ì •
        if patterns['sum_trend'] > 140:
            odd_candidates = [n for n in odd_candidates if n >= 15]
            even_candidates = [n for n in even_candidates if n >= 15]
        elif patterns['sum_trend'] < 120:
            odd_candidates = [n for n in odd_candidates if n <= 30]
            even_candidates = [n for n in even_candidates if n <= 30]

        # í™€ìˆ˜ ì„ íƒ
        if odd_candidates and target_odd > 0:
            selected.extend(random.sample(odd_candidates, min(target_odd, len(odd_candidates))))

        # ì§ìˆ˜ë¡œ ì±„ìš°ê¸°
        even_needed = 6 - len(selected)
        if even_candidates and even_needed > 0:
            remaining_evens = [n for n in even_candidates if n not in selected]
            selected.extend(random.sample(remaining_evens, min(even_needed, len(remaining_evens))))

        # ë¶€ì¡±í•˜ë©´ ëœë¤ ì±„ìš°ê¸°
        while len(selected) < 6:
            num = random.randint(1, 45)
            if num not in selected:
                selected.append(num)

        return selected[:6]

    def _similarity_prediction(self, train_data, seed):
        """ìœ ì‚¬ë„ ê¸°ë°˜ ì˜ˆì¸¡"""
        if len(train_data) < 2:
            return random.sample(range(1, 46), 6)

        last_numbers = set([train_data.iloc[-1][f'num{i}'] for i in range(1, 7)])
        
        similar_next_numbers = []
        similarity_thresholds = [0.33, 0.20, 0.15]

        for threshold in similarity_thresholds:
            threshold_numbers = []

            for i in range(max(0, len(train_data) - 30), len(train_data) - 1):
                if i >= 0:
                    compare_numbers = set([train_data.iloc[i][f'num{j}'] for j in range(1, 7)])
                    similarity = len(last_numbers & compare_numbers) / 6.0

                    if similarity >= threshold:
                        next_idx = i + 1
                        if next_idx < len(train_data):
                            next_numbers = [train_data.iloc[next_idx][f'num{k}'] for k in range(1, 7)]
                            threshold_numbers.extend(next_numbers)

            if threshold_numbers:
                similar_next_numbers.extend(threshold_numbers)
                break

        if similar_next_numbers:
            sim_counter = Counter(similar_next_numbers)
            candidates = [num for num, count in sim_counter.most_common(10)]

            if len(candidates) >= 6:
                selected = random.sample(candidates, 6)
            else:
                selected = candidates + random.sample([n for n in range(1, 46) if n not in candidates], 
                                                    6 - len(candidates))
        else:
            # ìœ ì‚¬ íŒ¨í„´ì´ ì—†ìœ¼ë©´ ìµœê·¼ ë¹ˆë„ ê¸°ë°˜
            recent_numbers = []
            for _, row in train_data.tail(10).iterrows():
                recent_numbers.extend([row[f'num{i}'] for i in range(1, 7)])

            freq_counter = Counter(recent_numbers)
            top_frequent = [num for num, count in freq_counter.most_common(12)]
            selected = random.sample(top_frequent, min(6, len(top_frequent)))

        # ë¶€ì¡±í•˜ë©´ ì±„ìš°ê¸°
        while len(selected) < 6:
            num = random.randint(1, 45)
            if num not in selected:
                selected.append(num)

        return selected[:6]

    def _statistical_prediction(self, train_data, seed):
        """í†µê³„ ê¸°ë°˜ ì˜ˆì¸¡"""
        recent_stats = train_data.tail(20)

        # ë‹¤ì¤‘ í†µê³„ ì§€í‘œ
        target_sum = recent_stats['sum_total'].mean() if 'sum_total' in recent_stats.columns else 130
        target_std = recent_stats['std_total'].mean() if 'std_total' in recent_stats.columns else 15

        selected = []
        mean_per_number = target_sum / 6

        # ì ì‘í˜• ë¶„í¬ ìƒì„±
        for i in range(6):
            if random.random() < 0.7:  # 70% ê°€ìš°ì‹œì•ˆ
                num = int(np.random.normal(mean_per_number, target_std))
            else:  # 30% ê· ë“±ë¶„í¬
                num = random.randint(1, 45)

            num = max(1, min(45, num))

            # ì¤‘ë³µ ë°©ì§€
            attempts = 0
            while num in selected and attempts < 20:
                if random.random() < 0.5:
                    num = int(np.random.normal(mean_per_number, target_std))
                else:
                    num = random.randint(1, 45)
                num = max(1, min(45, num))
                attempts += 1

            if num not in selected:
                selected.append(num)

        # ë¶€ì¡±í•˜ë©´ ì±„ìš°ê¸°
        while len(selected) < 6:
            num = random.randint(1, 45)
            if num not in selected:
                selected.append(num)

        return selected[:6]

    def _ml_prediction(self, train_data, seed):
        """ë¨¸ì‹ ëŸ¬ë‹ ê¸°ë°˜ ì˜ˆì¸¡"""
        if len(train_data) < 10:
            return random.sample(range(1, 46), 6)

        # ê°„ë‹¨í•œ íŠ¸ë Œë“œ ë¶„ì„
        recent_window = min(15, len(train_data))
        recent_data = train_data.tail(recent_window)

        trends = {}

        # í•©ê³„ ì¶”ì„¸
        if 'sum_total' in recent_data.columns and len(recent_data) >= 5:
            sum_values = recent_data['sum_total'].values
            trends['sum'] = np.polyfit(range(len(sum_values)), sum_values, 1)[0]
        else:
            trends['sum'] = 0

        # í™€ì§ ì¶”ì„¸
        if 'odd_count' in recent_data.columns and len(recent_data) >= 5:
            odd_values = recent_data['odd_count'].values
            trends['odd'] = np.polyfit(range(len(odd_values)), odd_values, 1)[0]
        else:
            trends['odd'] = 0

        selected = []

        # ì¶”ì„¸ ê¸°ë°˜ ë²ˆí˜¸ í’€ ìƒì„±
        if trends['sum'] > 0:  # í•©ê³„ ìƒìŠ¹ ì¶”ì„¸
            base_candidates = list(range(15, 46))
        elif trends['sum'] < 0:  # í•©ê³„ í•˜ë½ ì¶”ì„¸
            base_candidates = list(range(1, 31))
        else:  # ì¤‘ë¦½
            base_candidates = list(range(1, 46))

        # í™€ì§ ì¶”ì„¸ ë°˜ì˜
        if trends['odd'] > 0:  # í™€ìˆ˜ ì¦ê°€ ì¶”ì„¸
            odd_ratio = 0.6
        elif trends['odd'] < 0:  # í™€ìˆ˜ ê°ì†Œ ì¶”ì„¸
            odd_ratio = 0.4
        else:
            odd_ratio = 0.5

        # ì ì‘í˜• ì„ íƒ
        odd_candidates = [n for n in base_candidates if n % 2 == 1]
        even_candidates = [n for n in base_candidates if n % 2 == 0]

        target_odds = int(6 * odd_ratio)
        target_evens = 6 - target_odds

        # ë‹¤ì–‘ì„±ì„ ìœ„í•œ ëœë¤ ìƒ˜í”Œë§
        if odd_candidates and target_odds > 0:
            selected.extend(random.sample(odd_candidates, min(target_odds, len(odd_candidates))))

        if even_candidates and target_evens > 0:
            remaining_evens = [n for n in even_candidates if n not in selected]
            selected.extend(random.sample(remaining_evens, min(target_evens, len(remaining_evens))))

        # ë¶€ì¡±í•˜ë©´ ì±„ìš°ê¸°
        while len(selected) < 6:
            remaining = [n for n in range(1, 46) if n not in selected]
            if remaining:
                selected.append(random.choice(remaining))
            else:
                break

        return selected[:6]

    def number_performance_tracking_system(self):
        """ê°œì„ ë°©ì•ˆ #2: ë²ˆí˜¸ë³„ ì„±ê³¼ ì¶”ì  ì‹œìŠ¤í…œ"""
        logger.info("ë²ˆí˜¸ë³„ ì„±ê³¼ ì¶”ì  ì‹œìŠ¤í…œ êµ¬ì¶• ì¤‘...")

        # ë²ˆí˜¸ë³„ ê°œë³„ ì„±ê³¼ ë¶„ì„
        number_performance = {}

        for number in range(1, 46):
            performance_data = {
                'total_appearances': 0,
                'recent_appearances': 0,
                'hit_rate_overall': 0.0,
                'hit_rate_recent': 0.0,
                'trend': 'stable',
                'confidence': 0.0
            }

            # ì „ì²´ ì¶œí˜„ íšŸìˆ˜
            total_appearances = 0
            recent_appearances = 0  # ìµœê·¼ 30íšŒì°¨

            for i, row in self.historical_data.iterrows():
                numbers_in_draw = [row[f'num{j}'] for j in range(1, 7)]
                if number in numbers_in_draw:
                    total_appearances += 1

                    # ìµœê·¼ 30íšŒì°¨ì¸ì§€ í™•ì¸
                    if i >= len(self.historical_data) - 30:
                        recent_appearances += 1

            performance_data['total_appearances'] = total_appearances
            performance_data['recent_appearances'] = recent_appearances

            # ì „ì²´ ì ì¤‘ë¥ 
            total_draws = len(self.historical_data)
            performance_data['hit_rate_overall'] = total_appearances / total_draws if total_draws > 0 else 0

            # ìµœê·¼ ì ì¤‘ë¥ 
            recent_draws = min(30, total_draws)
            performance_data['hit_rate_recent'] = recent_appearances / recent_draws if recent_draws > 0 else 0

            # ì¶”ì„¸ ë¶„ì„
            if total_draws >= 60:
                old_appearances = total_appearances - recent_appearances
                old_draws = total_draws - recent_draws
                old_rate = old_appearances / old_draws if old_draws > 0 else 0
                recent_rate = performance_data['hit_rate_recent']

                if recent_rate > old_rate * 1.2:
                    performance_data['trend'] = 'rising'
                elif recent_rate < old_rate * 0.8:
                    performance_data['trend'] = 'falling'
                else:
                    performance_data['trend'] = 'stable'

            # ì‹ ë¢°ë„ ê³„ì‚°
            data_sufficiency = min(1.0, total_appearances / 15)
            rate_stability = 1.0 - abs(performance_data['hit_rate_recent'] - performance_data['hit_rate_overall'])
            performance_data['confidence'] = (data_sufficiency + rate_stability) / 2

            number_performance[number] = performance_data

        # ì„±ê³¼ ê¸°ë°˜ ë“±ê¸‰ ë¶„ë¥˜
        performance_grades = {
            'Sê¸‰': [],  # ìƒìœ„ 10%
            'Aê¸‰': [],  # ìƒìœ„ 11-25%
            'Bê¸‰': [],  # ìƒìœ„ 26-50%
            'Cê¸‰': [],  # ìƒìœ„ 51-75%
            'Dê¸‰': []   # í•˜ìœ„ 25%
        }

        # ì¢…í•© ì„±ê³¼ ì ìˆ˜ ê³„ì‚°
        for number, perf in number_performance.items():
            composite_score = (
                perf['hit_rate_recent'] * 0.4 +
                perf['confidence'] * 0.3 +
                (1 if perf['trend'] == 'rising' else 0.5 if perf['trend'] == 'stable' else 0) * 0.3
            )
            number_performance[number]['composite_score'] = composite_score

        # ì ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ë“±ê¸‰ ë¶„ë¥˜
        sorted_numbers = sorted(number_performance.items(), key=lambda x: x[1]['composite_score'], reverse=True)
        total_numbers = len(sorted_numbers)

        for i, (number, perf) in enumerate(sorted_numbers):
            percentile = i / total_numbers
            if percentile <= 0.1:
                performance_grades['Sê¸‰'].append(number)
            elif percentile <= 0.25:
                performance_grades['Aê¸‰'].append(number)
            elif percentile <= 0.5:
                performance_grades['Bê¸‰'].append(number)
            elif percentile <= 0.75:
                performance_grades['Cê¸‰'].append(number)
            else:
                performance_grades['Dê¸‰'].append(number)

        self.analysis_vault['number_performance'] = {
            'individual_performance': number_performance,
            'performance_grades': performance_grades,
            'top_performers': sorted_numbers[:10],
        }

        logger.info(f"ë²ˆí˜¸ë³„ ì„±ê³¼ ì¶”ì  ì™„ë£Œ - Sê¸‰: {performance_grades['Sê¸‰']}")

    def ultimate_ensemble_optimization(self):
        """ê¶ê·¹ì˜ ì•™ìƒë¸” ìµœì í™”"""
        logger.info("ê¶ê·¹ì˜ ì•™ìƒë¸” ìµœì í™” ì‹¤í–‰ ì¤‘...")

        # ëª¨ë“  ë¶„ì„ ê²°ê³¼ í†µí•©
        ensemble_components = {}

        # 1. í™•ì¥ ë°±í…ŒìŠ¤íŒ… ê²°ê³¼ í†µí•©
        if 'extended_backtesting' in self.analysis_vault:
            backtest_weights = {}
            methods_perf = self.analysis_vault['extended_backtesting'].get('methods_performance', {})
            total_score = sum(perf.get('composite_score', 1) for perf in methods_perf.values())

            for method, perf in methods_perf.items():
                backtest_weights[method] = perf.get('composite_score', 1) / total_score if total_score > 0 else 0.2

            ensemble_components['backtest_weights'] = backtest_weights

        # 2. ë²ˆí˜¸ë³„ ì„±ê³¼ í†µí•©
        if 'number_performance' in self.analysis_vault:
            performance_grades = self.analysis_vault['number_performance'].get('performance_grades', {})
            top_performers = self.analysis_vault['number_performance'].get('top_performers', [])

            ensemble_components['number_performance'] = {
                'S_grade_numbers': performance_grades.get('Sê¸‰', []),
                'A_grade_numbers': performance_grades.get('Aê¸‰', []),
                'top_10_numbers': [num for num, perf in top_performers]
            }

        # ê¶ê·¹ì˜ ê°€ì¤‘ì¹˜ ê³„ì‚°
        ultimate_weights = self._calculate_ultimate_weights(ensemble_components)

        self.analysis_vault['ultimate_ensemble'] = {
            'ultimate_weights': ultimate_weights,
            'ensemble_components': ensemble_components,
            'optimization_complete': True
        }

        logger.info("ê¶ê·¹ì˜ ì•™ìƒë¸” ìµœì í™” ì™„ë£Œ")

    def _calculate_ultimate_weights(self, ensemble_components):
        """ê¶ê·¹ì˜ ê°€ì¤‘ì¹˜ ê³„ì‚°"""
        # ê¸°ë³¸ ê°€ì¤‘ì¹˜
        base_weights = {
            'frequency_based': 0.20,
            'pattern_based': 0.20,
            'similarity_based': 0.20,
            'statistical_based': 0.20,
            'ml_based': 0.20
        }

        # ë°±í…ŒìŠ¤íŒ… ê°€ì¤‘ì¹˜ ë°˜ì˜
        if 'backtest_weights' in ensemble_components:
            backtest_weights = ensemble_components['backtest_weights']
            for method in base_weights:
                if method in backtest_weights:
                    base_weights[method] = backtest_weights[method] * 0.7 + base_weights[method] * 0.3

        # ì •ê·œí™”
        total_weight = sum(base_weights.values())
        if total_weight > 0:
            base_weights = {method: weight / total_weight for method, weight in base_weights.items()}

        return base_weights

    def generate_ultimate_predictions(self, count=1, user_numbers=None):
        """ê¶ê·¹ì˜ ì˜ˆì¸¡ ìƒì„±"""
        logger.info(f"ê¶ê·¹ì˜ ì˜ˆì¸¡ {count}ì„¸íŠ¸ ìƒì„± ì¤‘...")

        predictions = []
        used_combinations = set()

        # ê¶ê·¹ ê°€ì¤‘ì¹˜ ì‚¬ìš©
        ultimate_weights = self.analysis_vault.get('ultimate_ensemble', {}).get('ultimate_weights', {
            'frequency_based': 0.22,
            'pattern_based': 0.21,
            'similarity_based': 0.19,
            'statistical_based': 0.23,
            'ml_based': 0.15
        })

        strategies = ['ultimate_ensemble', 'performance_optimized', 'backtest_validated', 'condition_adapted']

        for i in range(count):
            strategy = strategies[i % len(strategies)]
            attempt = 0
            max_attempts = 100

            while attempt < max_attempts:
                attempt += 1

                selected = self._generate_strategy_set(strategy, ultimate_weights, i, user_numbers)

                if self._passes_quality_filters(selected):
                    combo_key = tuple(sorted(selected))
                    if combo_key not in used_combinations and len(selected) == 6:
                        used_combinations.add(combo_key)

                        quality_score = self._calculate_quality_score(selected)
                        expected_hits = self._calculate_expected_hits(selected)

                        predictions.append({
                            'set_id': i + 1,
                            'numbers': sorted(selected),
                            'quality_score': quality_score,
                            'confidence_level': self._get_confidence_level(quality_score),
                            'strategy': self._get_strategy_name(strategy),
                            'expected_hits': expected_hits,
                            'description': f'ê¶ê·¹ ì‹œìŠ¤í…œ #{i+1} - {strategy}'
                        })
                        break

        # ì¢…í•© ì ìˆ˜ìˆœ ì •ë ¬
        predictions.sort(key=lambda x: x['quality_score'], reverse=True)
        return predictions

    def _generate_strategy_set(self, strategy, ultimate_weights, seed, user_numbers):
        """ì „ëµë³„ ì„¸íŠ¸ ìƒì„±"""
        random.seed(42 + seed * 17)

        if strategy == 'ultimate_ensemble':
            return self._generate_ultimate_ensemble_set(ultimate_weights, seed, user_numbers)
        elif strategy == 'performance_optimized':
            return self._generate_performance_optimized_set(seed, user_numbers)
        elif strategy == 'backtest_validated':
            return self._generate_backtest_validated_set(seed, user_numbers)
        else:  # condition_adapted
            return self._generate_condition_adapted_set(seed, user_numbers)

    def _generate_ultimate_ensemble_set(self, ultimate_weights, seed, user_numbers):
        """ê¶ê·¹ ì•™ìƒë¸” ì„¸íŠ¸ ìƒì„±"""
        selected = []

        # ì‚¬ìš©ì ì„ í˜¸ ë²ˆí˜¸ ë¨¼ì € ì¶”ê°€
        if user_numbers:
            valid_user_numbers = [n for n in user_numbers if 1 <= n <= 45]
            selected.extend(valid_user_numbers[:2])  # ìµœëŒ€ 2ê°œê¹Œì§€

        # ê° ë°©ë²•ë¡ ë³„ ì˜ˆì¸¡ ìƒì„±
        method_predictions = {}
        method_predictions['frequency_based'] = self._frequency_prediction(self.historical_data, seed)
        method_predictions['pattern_based'] = self._pattern_prediction(self.historical_data, seed)
        method_predictions['similarity_based'] = self._similarity_prediction(self.historical_data, seed)
        method_predictions['statistical_based'] = self._statistical_prediction(self.historical_data, seed)
        method_predictions['ml_based'] = self._ml_prediction(self.historical_data, seed)

        # ê°€ì¤‘ì¹˜ ê¸°ë°˜ ë²ˆí˜¸ ì ìˆ˜ ê³„ì‚°
        number_scores = defaultdict(float)

        for method, predictions in method_predictions.items():
            weight = ultimate_weights.get(method, 0.2)
            for num in predictions:
                number_scores[num] += weight * 100

        # ìƒìœ„ ì ìˆ˜ ë²ˆí˜¸ë“¤ì—ì„œ ì„ íƒ
        sorted_numbers = sorted(number_scores.items(), key=lambda x: x[1], reverse=True)
        top_candidates = [num for num, score in sorted_numbers[:15]]

        # ì´ë¯¸ ì„ íƒëœ ë²ˆí˜¸ ì œì™¸í•˜ê³  ë‚˜ë¨¸ì§€ ì„ íƒ
        remaining_candidates = [n for n in top_candidates if n not in selected]
        needed = 6 - len(selected)
        
        if len(remaining_candidates) >= needed:
            selected.extend(random.sample(remaining_candidates, needed))
        else:
            selected.extend(remaining_candidates)
            # ë¶€ì¡±í•˜ë©´ ëœë¤ìœ¼ë¡œ ì±„ìš°ê¸°
            while len(selected) < 6:
                num = random.randint(1, 45)
                if num not in selected:
                    selected.append(num)

        return selected[:6]

    def _generate_performance_optimized_set(self, seed, user_numbers):
        """ì„±ê³¼ ìµœì í™” ì„¸íŠ¸ ìƒì„±"""
        selected = []

        # ì‚¬ìš©ì ë²ˆí˜¸ ì¶”ê°€
        if user_numbers:
            valid_user_numbers = [n for n in user_numbers if 1 <= n <= 45]
            selected.extend(valid_user_numbers[:2])

        if 'number_performance' in self.analysis_vault:
            performance_data = self.analysis_vault['number_performance']
            top_performers = performance_data.get('top_performers', [])
            S_grade = performance_data.get('performance_grades', {}).get('Sê¸‰', [])

            # Sê¸‰ ë²ˆí˜¸ì—ì„œ ìš°ì„  ì„ íƒ
            remaining_s_grade = [n for n in S_grade if n not in selected]
            if remaining_s_grade:
                selected.extend(random.sample(remaining_s_grade, min(2, len(remaining_s_grade))))

            # ìƒìœ„ ì„±ê³¼ìì—ì„œ ë³´ì¶©
            remaining_top = [num for num, perf in top_performers[:10] if num not in selected]
            if remaining_top and len(selected) < 6:
                selected.extend(random.sample(remaining_top, min(6 - len(selected), len(remaining_top))))

        # ë¶€ì¡±í•˜ë©´ ëœë¤ ì±„ìš°ê¸°
        while len(selected) < 6:
            num = random.randint(1, 45)
            if num not in selected:
                selected.append(num)

        return selected[:6]

    def _generate_backtest_validated_set(self, seed, user_numbers):
        """ë°±í…ŒìŠ¤íŠ¸ ê²€ì¦ ì„¸íŠ¸ ìƒì„±"""
        selected = []

        # ì‚¬ìš©ì ë²ˆí˜¸ ì¶”ê°€
        if user_numbers:
            valid_user_numbers = [n for n in user_numbers if 1 <= n <= 45]
            selected.extend(valid_user_numbers[:2])

        if 'extended_backtesting' in self.analysis_vault:
            backtest_data = self.analysis_vault['extended_backtesting']
            best_method = backtest_data.get('best_method', 'statistical_based')

            # ìµœê³  ì„±ê³¼ ë°©ë²•ë¡ ìœ¼ë¡œ ì˜ˆì¸¡
            remaining_slots = 6 - len(selected)
            method_prediction = self._backtest_predict_method(self.historical_data, best_method, seed)
            remaining_prediction = [n for n in method_prediction if n not in selected]
            
            selected.extend(remaining_prediction[:remaining_slots])

        # ë¶€ì¡±í•˜ë©´ ì±„ìš°ê¸°
        while len(selected) < 6:
            num = random.randint(1, 45)
            if num not in selected:
                selected.append(num)

        return selected[:6]

    def _generate_condition_adapted_set(self, seed, user_numbers):
        """ì¡°ê±´ ì ì‘ ì„¸íŠ¸ ìƒì„±"""
        # ê¸°ë³¸ì ìœ¼ë¡œ í†µê³„ ê¸°ë°˜ ì˜ˆì¸¡ ì‚¬ìš©
        return self._statistical_prediction(self.historical_data, seed)

    def _passes_quality_filters(self, numbers):
        """í’ˆì§ˆ í•„í„° í†µê³¼ ì—¬ë¶€"""
        if len(numbers) != 6:
            return False

        total_sum = sum(numbers)
        odd_count = sum(1 for n in numbers if n % 2 == 1)
        high_count = sum(1 for n in numbers if n >= 23)
        number_range = max(numbers) - min(numbers)

        quality_checks = [
            100 <= total_sum <= 200,
            1 <= odd_count <= 5,
            1 <= high_count <= 5,
            15 <= number_range <= 40,
            len(set(numbers)) == 6
        ]

        return all(quality_checks)

    def _calculate_quality_score(self, numbers):
        """í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°"""
        if len(numbers) != 6:
            return 0

        score = 0

        # ê¸°ë³¸ ì¡°í™”ì„± ì ìˆ˜
        total_sum = sum(numbers)
        odd_count = sum(1 for n in numbers if n % 2 == 1)
        high_count = sum(1 for n in numbers if n >= 23)
        number_range = max(numbers) - min(numbers)

        # í•©ê³„ ì ìˆ˜
        if 130 <= total_sum <= 170:
            score += 200
        elif 120 <= total_sum <= 180:
            score += 150

        # í™€ì§ ê· í˜• ì ìˆ˜
        if 2 <= odd_count <= 4:
            score += 200

        # ê³ ì € ê· í˜• ì ìˆ˜
        if 2 <= high_count <= 4:
            score += 200

        # ë¶„í¬ ë²”ìœ„ ì ìˆ˜
        if 20 <= number_range <= 35:
            score += 100

        # ë²ˆí˜¸ë³„ ì„±ê³¼ ì ìˆ˜
        if 'number_performance' in self.analysis_vault:
            performance_data = self.analysis_vault['number_performance']['individual_performance']
            for num in numbers:
                if num in performance_data:
                    perf = performance_data[num]
                    score += perf.get('composite_score', 0.5) * 100

        return score

    def _calculate_expected_hits(self, numbers):
        """ì˜ˆìƒ ì ì¤‘ ê°œìˆ˜ ê³„ì‚°"""
        if len(numbers) != 6:
            return 1.0

        expected_hits = 0.8  # ê¸°ë³¸ ê¸°ëŒ“ê°’

        # ë²ˆí˜¸ë³„ ê°œë³„ ì„±ê³¼ ê¸°ë°˜
        if 'number_performance' in self.analysis_vault:
            performance_data = self.analysis_vault['number_performance']['individual_performance']

            for num in numbers:
                if num in performance_data:
                    perf = performance_data[num]
                    individual_expectation = (
                        perf.get('hit_rate_recent', 0.13) * 0.6 +
                        perf.get('confidence', 0.5) * 0.13 * 0.4
                    )
                    expected_hits += individual_expectation * 0.1

        # ì¡°í•© ì‹œë„ˆì§€ íš¨ê³¼
        harmony_bonus = 0
        odd_count = sum(1 for num in numbers if num % 2 == 1)
        high_count = sum(1 for num in numbers if num >= 23)
        total_sum = sum(numbers)
        
        if 2 <= odd_count <= 4:
            harmony_bonus += 0.05
        if 2 <= high_count <= 4:
            harmony_bonus += 0.05
        if 120 <= total_sum <= 180:
            harmony_bonus += 0.1

        expected_hits += harmony_bonus

        # í˜„ì‹¤ì  ë²”ìœ„ ì œí•œ
        expected_hits = max(0.8, min(1.8, expected_hits))

        return round(expected_hits, 2)

    def _get_confidence_level(self, quality_score):
        """ì‹ ë¢°ë„ ë ˆë²¨"""
        if quality_score >= 1000:
            return "ğŸ† Ultimate Legend"
        elif quality_score >= 800:
            return "â­ Supreme Master"
        elif quality_score >= 600:
            return "ğŸ’ Premium Elite"
        elif quality_score >= 400:
            return "ğŸš€ Advanced Pro"
        else:
            return "ğŸ“Š Standard Quality"

    def _get_strategy_name(self, strategy):
        """ì „ëµëª… ë³€í™˜"""
        strategy_names = {
            'ultimate_ensemble': 'ê¶ê·¹ì•™ìƒë¸”',
            'performance_optimized': 'ì„±ê³¼ìµœì í™”',
            'backtest_validated': 'ë°±í…ŒìŠ¤íŠ¸ê²€ì¦',
            'condition_adapted': 'ì¡°ê±´ì ì‘'
        }
        return strategy_names.get(strategy, strategy)

    def predict(self, count=1, user_numbers=None):
        """ì›¹ì•±ìš© í†µí•© ì˜ˆì¸¡ ë©”ì„œë“œ"""
        try:
            result = {
                'success': False,
                'algorithm': self.algorithm_info['name'],
                'version': self.algorithm_info['version'],
                'predictions': [],
                'metadata': {},
                'error': None,
                'execution_time': 0
            }

            start_time = datetime.now()

            # 1. ë°ì´í„° ë¡œë“œ
            self.historical_data = self._load_and_enhance_data('data/new_1190.csv')
            if self.historical_data.empty:
                result['error'] = 'ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨'
                return result

            # 2. í™•ì¥ ë°±í…ŒìŠ¤íŒ… ì‹œìŠ¤í…œ
            self.extended_backtesting_system()

            # 3. ë²ˆí˜¸ë³„ ì„±ê³¼ ì¶”ì  ì‹œìŠ¤í…œ
            self.number_performance_tracking_system()

            # 4. ê¶ê·¹ ì•™ìƒë¸” ìµœì í™”
            self.ultimate_ensemble_optimization()

            # 5. ê¶ê·¹ì˜ ì˜ˆì¸¡ ìƒì„±
            predictions = self.generate_ultimate_predictions(count=count, user_numbers=user_numbers)

            if not predictions:
                result['error'] = 'ì˜ˆì¸¡ ìƒì„± ì‹¤íŒ¨'
                return result

            result['predictions'] = predictions

            # ë©”íƒ€ë°ì´í„° ì¶”ê°€
            result['metadata'] = {
                'data_rounds': len(self.historical_data),
                'backtesting_periods': self.analysis_vault.get('extended_backtesting', {}).get('backtest_periods', 0),
                'best_method': self.analysis_vault.get('extended_backtesting', {}).get('best_method', 'N/A'),
                'S_grade_numbers': len(self.analysis_vault.get('number_performance', {}).get('performance_grades', {}).get('Sê¸‰', [])),
                'analysis_completeness': len(self.analysis_vault),
                'ultimate_system': True
            }

            end_time = datetime.now()
            result['execution_time'] = (end_time - start_time).total_seconds()
            result['success'] = True

            logger.info(f"âœ… Ultimate ì˜ˆì¸¡ ì™„ë£Œ: {count}ì„¸íŠ¸, {result['execution_time']:.2f}ì´ˆ")

            return result

        except Exception as e:
            logger.error(f"Ultimate ì˜ˆì¸¡ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            return {
                'success': False,
                'algorithm': self.algorithm_info['name'],
                'version': self.algorithm_info['version'],
                'predictions': [],
                'metadata': {},
                'error': str(e),
                'execution_time': 0
            }

# ì›¹ì•± ì‹¤í–‰ì„ ìœ„í•œ í¸ì˜ í•¨ìˆ˜
def run_ultimate_system_v1(data_path='data/new_1190.csv', count=1, user_numbers=None):
    """ì›¹ì•±ì—ì„œ í˜¸ì¶œí•  ìˆ˜ ìˆëŠ” ì‹¤í–‰ í•¨ìˆ˜"""
    predictor = UltimateLottoPredictionSystemV1()
    return predictor.predict(count=count, user_numbers=user_numbers)

# ì•Œê³ ë¦¬ì¦˜ ì •ë³´ ì¡°íšŒ í•¨ìˆ˜
def get_algorithm_info():
    """ì•Œê³ ë¦¬ì¦˜ ì •ë³´ ë°˜í™˜"""
    predictor = UltimateLottoPredictionSystemV1()
    return predictor.get_algorithm_info()

if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    result = run_ultimate_system_v1(count=2)
    print(json.dumps(result, indent=2, ensure_ascii=False))